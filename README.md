<div align="center">

# Deep Learning Sandbox

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![Lightning](https://img.shields.io/badge/Lightning-2.0+-792ee5.svg)](https://lightning.ai/)

*A personal learning space for exploring deep learning through clean PyTorch implementations*

**Topics:** `deep-learning` `pytorch` `machine-learning` `neural-networks` `computer-vision` `natural-language-processing` `transformers` `resnet` `gpt-2` `clip` `siamese-networks` `pytorch-lightning` `optuna` `hyperparameter-optimization`

</div>

---

## What's This About?

This repository is my journey through deep learning research and implementation. I study papers, experiment with implementations, document my understanding, and build clean PyTorch code. It's a sandbox where theory meets practice.

**The Flow:** Read → Experiment → Document → Implement

---

## What You'll Find Here

### Contrastive Language-Image Pretraining (CLIP)
Multimodal learning that connects vision and language. Implementation of OpenAI's CLIP model for zero-shot image classification and image-text matching.

### Generative Pre-trained Transformer 2 (GPT-2)
Decoder-only transformer for text generation. Autoregressive language modeling with attention mechanisms and byte-pair encoding tokenization.

### Residual Network (ResNet)
Deep convolutional networks with skip connections. Implementation of ResNet architectures for image classification, solving the vanishing gradient problem.

### Siamese Network
Twin neural networks for similarity learning. Used for one-shot learning, face verification, and metric learning tasks.

### Transformers
Core transformer architecture from "Attention Is All You Need". Building blocks including multi-head attention, positional encoding, and encoder-decoder stacks.

### References
Collection of research papers, implementation notes, and learning resources that guide these implementations.

---

## Built With

- **PyTorch** - Deep learning framework
- **PyTorch Lightning** - High-level PyTorch wrapper for cleaner training code
- **Optuna** - Hyperparameter optimization framework
- **TensorBoard** - Training visualization and monitoring
- **uv** - Modern Python package management

Each implementation folder contains its own documentation with detailed explanations and usage examples.

---

## Getting Started

```bash
# Clone the repository
git clone https://github.com/Siddharth-magesh/deep-learning-sandbox.git
cd deep-learning-sandbox

# Install dependencies
uv sync

# Start exploring!
```

---

## Contributing

This is primarily a personal learning repository, but if you spot issues or have suggestions, feel free to open an issue or submit a pull request. Contributions that improve clarity, fix bugs, or add documentation are always welcome!

---

## License

MIT License - feel free to use this code for learning and experimentation.

---

<div align="center">

**Made with care while learning deep learning**

[Star this repo](https://github.com/Siddharth-magesh/deep-learning-sandbox) if you find it helpful!

</div>
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3754e828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 11:18:33 - INFO - ======================================================================\n",
      "2025-12-03 11:18:33 - INFO - BRAIN IMAGE SCRAPER PIPELINE\n",
      "2025-12-03 11:18:33 - INFO - ======================================================================\n",
      "2025-12-03 11:18:33 - INFO - URLs to process: 1\n",
      "2025-12-03 11:18:33 - INFO - Output directory: brain_dataset_test1\n",
      "2025-12-03 11:18:33 - INFO - AI threshold: 0.5\n",
      "2025-12-03 11:18:33 - INFO - ======================================================================\n",
      "2025-12-03 11:18:33 - INFO - \n",
      "======================================================================\n",
      "2025-12-03 11:18:33 - INFO - STAGE 1: EXTRACTION\n",
      "2025-12-03 11:18:33 - INFO - ======================================================================\n",
      "2025-12-03 11:18:36 - INFO - Extracted 64 images from https://histology.siu.edu/ssb/neuron.htm\n",
      "2025-12-03 11:18:36 - INFO - Total extracted: 64 images from 1 sources\n",
      "2025-12-03 11:18:36 - INFO - Extracted 64 images from 1 sources\n",
      "2025-12-03 11:18:36 - INFO - \n",
      "======================================================================\n",
      "2025-12-03 11:18:36 - INFO - STAGE 2: FILTERING\n",
      "2025-12-03 11:18:36 - INFO - ======================================================================\n",
      "2025-12-03 11:18:38 - INFO - Running rule-based filtering...\n",
      "2025-12-03 11:18:38 - INFO - Passed rules: 61, Failed: 3\n",
      "2025-12-03 11:18:38 - INFO - Downloading images...\n",
      "2025-12-03 11:19:46 - INFO - Download progress: 50/61\n",
      "2025-12-03 11:20:02 - INFO - Downloaded: 61, Failed: 0\n",
      "2025-12-03 11:20:02 - INFO - Running AI classification...\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Disabling PyTorch because PyTorch >= 2.1 is required but found 2.1.0a0+32f93b1\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "2025-12-03 11:20:10 - INFO - Loading CLIP model on cuda...\n",
      "2025-12-03 11:20:10 - INFO - Parsing model identifier. Schema: None, Identifier: ViT-H-14\n",
      "2025-12-03 11:20:10 - INFO - Loaded built-in ViT-H-14 model config.\n",
      "2025-12-03 11:20:10 - INFO - Instantiating model architecture: CLIP\n",
      "2025-12-03 11:20:19 - INFO - Loading full pretrained weights from: /home/siddharth/.cache/huggingface/hub/models--laion--CLIP-ViT-H-14-laion2B-s32B-b79K/snapshots/1c2b8495b28150b8a4922ee1c8edee224c284c0c/open_clip_model.safetensors\n",
      "2025-12-03 11:20:37 - INFO - Final image preprocessing configuration set: {'size': (224, 224), 'mode': 'RGB', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'interpolation': 'bicubic', 'resize_mode': 'shortest', 'fill_color': 0}\n",
      "2025-12-03 11:20:37 - INFO - Model ViT-H-14 creation process complete.\n",
      "2025-12-03 11:20:37 - INFO - Loaded primary model: ViT-H-14\n",
      "2025-12-03 11:20:37 - INFO - Parsing tokenizer identifier. Schema: None, Identifier: ViT-H-14\n",
      "2025-12-03 11:20:37 - INFO - Attempting to load config from built-in: ViT-H-14\n",
      "2025-12-03 11:20:37 - INFO - Using default SimpleTokenizer.\n",
      "2025-12-03 11:20:40 - INFO - AI classification completed in 1.8s\n",
      "2025-12-03 11:20:40 - INFO - Brain images: 61, Non-brain: 0\n",
      "2025-12-03 11:20:40 - INFO - Filtered to 61 brain images\n",
      "2025-12-03 11:20:40 - INFO - \n",
      "======================================================================\n",
      "2025-12-03 11:20:40 - INFO - STAGE 3: LABELING\n",
      "2025-12-03 11:20:40 - INFO - ======================================================================\n",
      "2025-12-03 11:20:40 - INFO - Loading CLIP model for labeling on cuda...\n",
      "2025-12-03 11:20:40 - INFO - Parsing model identifier. Schema: None, Identifier: ViT-H-14\n",
      "2025-12-03 11:20:40 - INFO - Loaded built-in ViT-H-14 model config.\n",
      "2025-12-03 11:20:40 - INFO - Instantiating model architecture: CLIP\n",
      "2025-12-03 11:20:48 - INFO - Loading full pretrained weights from: /home/siddharth/.cache/huggingface/hub/models--laion--CLIP-ViT-H-14-laion2B-s32B-b79K/snapshots/1c2b8495b28150b8a4922ee1c8edee224c284c0c/open_clip_model.safetensors\n",
      "2025-12-03 11:20:49 - INFO - Final image preprocessing configuration set: {'size': (224, 224), 'mode': 'RGB', 'mean': (0.48145466, 0.4578275, 0.40821073), 'std': (0.26862954, 0.26130258, 0.27577711), 'interpolation': 'bicubic', 'resize_mode': 'shortest', 'fill_color': 0}\n",
      "2025-12-03 11:20:49 - INFO - Model ViT-H-14 creation process complete.\n",
      "2025-12-03 11:20:49 - INFO - Parsing tokenizer identifier. Schema: None, Identifier: ViT-H-14\n",
      "2025-12-03 11:20:49 - INFO - Attempting to load config from built-in: ViT-H-14\n",
      "2025-12-03 11:20:49 - INFO - Using default SimpleTokenizer.\n",
      "2025-12-03 11:20:49 - INFO - Generating AI labels for 61 images...\n",
      "2025-12-03 11:20:53 - INFO - Generating final labels...\n",
      "2025-12-03 11:20:53 - INFO - Labeling complete\n",
      "2025-12-03 11:20:53 - INFO - Labeled 61 images\n",
      "2025-12-03 11:20:53 - INFO - \n",
      "======================================================================\n",
      "2025-12-03 11:20:53 - INFO - STAGE 4: STORAGE\n",
      "2025-12-03 11:20:53 - INFO - ======================================================================\n",
      "2025-12-03 11:20:53 - INFO - Saving 61 images...\n",
      "2025-12-03 11:20:53 - INFO - Save progress: 30/61\n",
      "2025-12-03 11:20:53 - INFO - Save progress: 60/61\n",
      "2025-12-03 11:20:53 - INFO - Saved 61 images to brain_dataset_test1/images\n",
      "2025-12-03 11:20:53 - INFO - Exporting data...\n",
      "2025-12-03 11:20:53 - INFO - Exported JSON: brain_dataset_test1/dataset.json (91KB)\n",
      "2025-12-03 11:20:53 - INFO - Exported CSV: brain_dataset_test1/dataset.csv (37KB)\n",
      "2025-12-03 11:20:53 - INFO - Exported labels: brain_dataset_test1/labels.csv\n",
      "2025-12-03 11:20:53 - INFO - Exported summary: brain_dataset_test1/summary.txt\n",
      "2025-12-03 11:20:53 - INFO - Data exported to brain_dataset_test1\n",
      "2025-12-03 11:20:53 - INFO - \n",
      "======================================================================\n",
      "2025-12-03 11:20:53 - INFO - PIPELINE COMPLETE\n",
      "2025-12-03 11:20:53 - INFO - ======================================================================\n",
      "2025-12-03 11:20:53 - INFO - Total time: 140.3s\n",
      "2025-12-03 11:20:53 - INFO - \n",
      "PIPELINE STATISTICS\n",
      "──────────────────────────────────────────────────\n",
      "  Extracted:          64 images\n",
      "  Passed Rules:       61 images\n",
      "  Downloaded:         61 images\n",
      "  Brain Images:       61 images\n",
      "  Saved:              61 images\n",
      "\n",
      "FILTERING DETAILS\n",
      "──────────────────────────────────────────────────\n",
      "  Extension filter:     2 removed\n",
      "  Pattern filter:       1 removed\n",
      "  Download failed:      0 failed\n",
      "  AI filter:            0 removed\n",
      "\n",
      "LABEL CONFIDENCE\n",
      "──────────────────────────────────────────────────\n",
      "  High:        60 (98.4%)\n",
      "  Medium:       1 (1.6%)\n",
      "  Low:          0 (0.0%)\n",
      "\n",
      "2025-12-03 11:20:53 - INFO - \n",
      "Output files:\n",
      "2025-12-03 11:20:53 - INFO -   json: brain_dataset_test1/dataset.json\n",
      "2025-12-03 11:20:53 - INFO -   csv: brain_dataset_test1/dataset.csv\n",
      "2025-12-03 11:20:53 - INFO -   labels: brain_dataset_test1/labels.csv\n",
      "2025-12-03 11:20:53 - INFO -   summary: brain_dataset_test1/summary.txt\n"
     ]
    }
   ],
   "source": [
    "from pipeline import run_pipeline\n",
    "\n",
    "result = run_pipeline(\n",
    "    urls=['https://histology.siu.edu/ssb/neuron.htm'],\n",
    "    output_dir='./brain_dataset_test1',\n",
    "    threshold=0.50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a430c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "479a2ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "df24be88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignatureTripletDataset(Dataset):\n",
    "    def __init__(self, base_data_dir, triplets_per_user=100, transform=None, signature_map=None, user_ids=None):\n",
    "        self.base_data_dir = base_data_dir\n",
    "        self.triplets_per_user = triplets_per_user\n",
    "        self.transform = transform\n",
    "        \n",
    "        if signature_map is None:\n",
    "            self.signature_map = self._create_signature_map()\n",
    "        else:\n",
    "            self.signature_map = signature_map\n",
    "            \n",
    "        if user_ids is None:\n",
    "            self.user_ids = sorted(list(self.signature_map.keys()))\n",
    "        else:\n",
    "            self.user_ids = list(user_ids)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.user_ids) * self.triplets_per_user\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        user_id = random.choice(self.user_ids)\n",
    "        \n",
    "        real_paths = self.signature_map[user_id][\"real\"]\n",
    "        anchor_path, positive_path = random.sample(real_paths, 2)\n",
    "        \n",
    "        fake_paths = self.signature_map[user_id][\"fake\"]\n",
    "        negative_path = random.choice(fake_paths)\n",
    "        \n",
    "        anchor_img = self._load_image(anchor_path)\n",
    "        positive_img = self._load_image(positive_path)\n",
    "        negative_img = self._load_image(negative_path)\n",
    "        \n",
    "        return anchor_img, positive_img, negative_img\n",
    "    \n",
    "    def _create_signature_map(self):\n",
    "        signature_map = {}\n",
    "        \n",
    "        real_dir = os.path.join(self.base_data_dir, \"Real\")\n",
    "        fake_dir = os.path.join(self.base_data_dir, \"Fake\")\n",
    "        \n",
    "        for user_id in os.listdir(real_dir):\n",
    "            user_real_dir = os.path.join(real_dir, user_id)\n",
    "            if not os.path.isdir(user_real_dir):\n",
    "                continue\n",
    "            \n",
    "            real_paths = [\n",
    "                os.path.join(user_real_dir, f)\n",
    "                for f in os.listdir(user_real_dir)\n",
    "                if f.lower().endswith((\".png\",\".jpg\",\".jpeg\"))\n",
    "            ]\n",
    "            \n",
    "            user_fake_dir = os.path.join(fake_dir, user_id)\n",
    "            fake_paths = []\n",
    "            if os.path.isdir(user_fake_dir):\n",
    "                fake_paths = [\n",
    "                    os.path.join(user_fake_dir, f)\n",
    "                    for f in os.listdir(user_fake_dir)\n",
    "                    if f.lower().endswith((\".png\",\".jpg\",\"jpeg\"))\n",
    "                ]\n",
    "            if len(real_paths) >= 2 and len(fake_paths) >= 1:\n",
    "                signature_map[user_id] = {\n",
    "                    \"real\" : real_paths,\n",
    "                    \"fake\" : fake_paths\n",
    "                }\n",
    "        return signature_map\n",
    "    \n",
    "    def _load_image(self, path):\n",
    "        with Image.open(path) as img:\n",
    "            img = img.convert(\"RGB\")\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "747ad662",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.861, 0.861, 0.861]\n",
    "std = [0.274, 0.274, 0.274]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomAffine(\n",
    "        degrees=0,\n",
    "        shear=10,\n",
    "        translate=(0.1,0.1)\n",
    "    ),\n",
    "    transforms.RandomPerspective(distortion_scale=0.1, p=0.5),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bde697e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_data_dir = \"/home/siddharth/workspace/src/brain_scraper/Signature_Verification_v5_v11/\"\n",
    "full_dataset = SignatureTripletDataset(\n",
    "    base_data_dir=signature_data_dir,\n",
    "    triplets_per_user=100,\n",
    "    transform=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ffaad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_signature_datasets_splits(full_dataset, train_split=0.8, train_transform=None, val_transform=None):\n",
    "    all_users = full_dataset.user_ids.copy()\n",
    "    random.shuffle(all_users)\n",
    "    \n",
    "    n_train = int(len(all_users) * train_split)\n",
    "    train_users = all_users[:n_train]\n",
    "    val_users = all_users[n_train:]\n",
    "    \n",
    "    train_dataset = SignatureTripletDataset(\n",
    "        base_data_dir=full_dataset.base_data_dir,\n",
    "        triplets_per_user=full_dataset.triplets_per_user,\n",
    "        transform=train_transform,\n",
    "        signature_map=full_dataset.signature_map,\n",
    "        user_ids=train_users\n",
    "    )\n",
    "    \n",
    "    val_dataset = SignatureTripletDataset(\n",
    "        base_data_dir=full_dataset.base_data_dir,\n",
    "        triplets_per_user=full_dataset.triplets_per_user,\n",
    "        transform=val_transform,\n",
    "        signature_map=full_dataset.signature_map,\n",
    "        user_ids=val_users\n",
    "    )\n",
    "    \n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c29cbcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = create_signature_datasets_splits(\n",
    "    full_dataset=full_dataset,\n",
    "    train_split=0.8,\n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d60bfe72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 4000\n",
      "Val Size: 1100\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(\"Train Size:\", len(train_dataset))\n",
    "print(\"Val Size:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2a00ba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEmbeddingNetwork(nn.Module):\n",
    "    def __init__(self, embedding_dim=128):\n",
    "        super(SimpleEmbeddingNetwork, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features=128 * 25 * 25, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(in_features=256, out_features=embedding_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "96e5eb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, embedding_network):\n",
    "        super().__init__()\n",
    "        self.embedding_network = embedding_network\n",
    "        \n",
    "    def forward(self, *inputs, triplet_bool=True):\n",
    "        if triplet_bool:\n",
    "            anchor, positive, negative = inputs\n",
    "            z_a = self.embedding_network(anchor)\n",
    "            z_p = self.embedding_network(positive)\n",
    "            z_n = self.embedding_network(negative)\n",
    "            return z_a, z_p, z_n\n",
    "        else:\n",
    "            img1, img2 = inputs\n",
    "            z1 = self.embedding_network(img1)\n",
    "            z2 = self.embedding_network(img2)\n",
    "            return z1, z2\n",
    "    \n",
    "    def get_embedding(self, image):\n",
    "        return self.embedding_network(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "31090455",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=128\n",
    "embedding_net = SimpleEmbeddingNetwork(embedding_dim=embedding_dim)\n",
    "siamese_model = SiameseNetwork(embedding_network=embedding_net).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b7233acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "optimizer = optim.Adam(params=siamese_model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b4a501c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_signature(model, train_loader, val_loader, loss_fcn, optimizer, scheduler, num_epochs, threshold, device):\n",
    "    best_val_acc = 0.0\n",
    "    best_state_dict = None\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            anchor, positive, negative = batch\n",
    "            anchor = anchor.to(device)\n",
    "            positive = positive.to(device)\n",
    "            negative = negative.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            z_a, z_p, z_n = model(anchor, positive, negative, triplet_bool=True)\n",
    "            loss = loss_fcn(z_a, z_p, z_n)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * anchor.size(0)\n",
    "        scheduler.step()\n",
    "        avg_train_loss = running_loss / len(train_loader.dataset)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                anchor, positive, negative = batch\n",
    "                anchor = anchor.to(device)\n",
    "                positive = positive.to(device)\n",
    "                negative = negative.to(device)\n",
    "                \n",
    "                z_a, z_p, z_n = model(anchor, positive, negative, triplet_bool=True)\n",
    "                loss = loss_fcn(z_a, z_p, z_n)\n",
    "                val_loss += loss.item() * anchor.size(0)\n",
    "                \n",
    "                d_ap = F.pairwise_distance(z_a, z_p)\n",
    "                d_an = F.pairwise_distance(z_a, z_n)\n",
    "                \n",
    "                genuine_correct = (d_ap < threshold)\n",
    "                fake_correct = (d_an >= threshold)\n",
    "                batch_correct = (genuine_correct & fake_correct).sum().item()\n",
    "                \n",
    "                correct += batch_correct\n",
    "                total += anchor.size(0)\n",
    "                \n",
    "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_acc = correct / total if total > 0 else 0.0\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}  \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f}  Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_state_dict = model.state_dict()\n",
    "    \n",
    "    if best_state_dict is not None:\n",
    "        model.load_state_dict(best_state_dict)\n",
    "    \n",
    "    print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "383af5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] Train Loss: 0.5894  Val Loss: 0.5630  Val Acc: 0.3791\n",
      "Epoch [2/5] Train Loss: 0.4095  Val Loss: 0.5685  Val Acc: 0.4209\n",
      "Epoch [3/5] Train Loss: 0.3084  Val Loss: 0.4957  Val Acc: 0.3073\n",
      "Epoch [4/5] Train Loss: 0.2971  Val Loss: 0.5490  Val Acc: 0.4191\n",
      "Epoch [5/5] Train Loss: 0.2734  Val Loss: 0.5183  Val Acc: 0.3909\n",
      "Best validation accuracy: 0.4209\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "threshold_dist = 0.8  # you can tune this later\n",
    "\n",
    "trained_siamese = training_loop_signature(\n",
    "    model=siamese_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    loss_fcn=triplet_loss,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=num_epochs,\n",
    "    threshold=threshold_dist,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ad2fd28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_triplet(data_loader):\n",
    "    model_batch = next(iter(data_loader))\n",
    "    anchor, positive, negative = model_batch\n",
    "    idx = random.randint(0, anchor.size(0), -1)\n",
    "    \n",
    "    imgs = [anchor[idx], positive[idx], negative[idx]]\n",
    "    titles = [\"Anchor (Real)\", \"Positive (Real)\", \"Negative (Fake)\"]\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    for i, (img, title) in enumerate(zip(imgs, titles), 1):\n",
    "        plt.subplot(1, 3, i)\n",
    "        img_np = img.permute(1, 2, 0).cpu.numpy()\n",
    "        img_np = img_np * std[0] + mean[0]\n",
    "        img_np = img_np.clip(0, 1)\n",
    "        plt.imshow(img_np, cmap=\"gray\")\n",
    "        plt.title(title)\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "22f4a67c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Random.randint() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mshow_random_triplet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[54], line 4\u001b[0m, in \u001b[0;36mshow_random_triplet\u001b[0;34m(data_loader)\u001b[0m\n\u001b[1;32m      2\u001b[0m model_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(data_loader))\n\u001b[1;32m      3\u001b[0m anchor, positive, negative \u001b[38;5;241m=\u001b[39m model_batch\n\u001b[0;32m----> 4\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manchor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m imgs \u001b[38;5;241m=\u001b[39m [anchor[idx], positive[idx], negative[idx]]\n\u001b[1;32m      7\u001b[0m titles \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnchor (Real)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPositive (Real)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNegative (Fake)\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: Random.randint() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "show_random_triplet(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9cc51b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
